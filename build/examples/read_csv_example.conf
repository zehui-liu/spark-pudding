job {
  appName = "spark read csv example job"
  describe = "spark test job"
  enableHiveSupport = "true"
}
sink = [
  {
    config {
      isStreaming = false
      numRows = 10
      printSchema = true
      truncate = 20
      vertical = false
    }
    dependency = source-1
    describe = "show data on console"
    id = sink1
    type = console-sink
  },
  {
    config {
      fileType = orc
      options {
        delimiter = ","
        header = "true"
      }
      outputMode = overwrite
      path = "hdfs:///user/hive/test001"
      saveMode = append
      streamingWriter = true
      timeoutMs = 200
    }
    dependency = source-1
    describe = "file common source,local file or hdfs file"
    id = sink2
    type = file-sink
  }
]
source = [
  {
    config {
      fileType = csv
      forceConvertSchema = true
      options {
        delimiter = ","
        header = "true"
      }
      path = "hdfs:///user/liuzehui/spark-pudding/examples/data/demo.csv"
      schema {
        col1 = string
        col2 = string
        col3 = string
        col4 = string
      }
      streamRead = false
      textDelimiter = ","
    }
    describe = "file common source,local file or hdfs file"
    id = source-1
    type = file-common-source
  }
]